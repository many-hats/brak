{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from brak.model import * \n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "clean_data_root = Path('/home/jupyter/fastai_audio/VoxCeleb1/img/')\n",
    "run_id = 'gru-mish-ranger-256-3-layer'\n",
    "models_dir = Path('/home/jupyter/fastai_audio/VoxCeleb1/models/')\n",
    "state_fpath = models_dir.joinpath(run_id + \".pt\")\n",
    "backup_dir = models_dir.joinpath(run_id + \"_backups\")\n",
    "vis_every=100\n",
    "umap_every=100\n",
    "save_every=100\n",
    "backup_every=1000\n",
    "visdom_server='http://localhost'\n",
    "no_visdom=True\n",
    "force_restart=False\n",
    "# def sync(device: torch.device):\n",
    "#     # FIXME\n",
    "#     return \n",
    "#     # For correct profiling (cuda operations are async)\n",
    "#     if device.type == \"cuda\":\n",
    "#         torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def train_voxceleb1(run_id: str, clean_data_root: Path, models_dir: Path, umap_every: int, save_every: int,\n",
    "          backup_every: int, vis_every: int, force_restart: bool, visdom_server: str,\n",
    "          no_visdom: bool):\n",
    "    dataset = SpeakerVerificationDataset(clean_data_root)\n",
    "    loader = SpeakerVerificationDataLoader(\n",
    "        dataset,\n",
    "        speakers_per_batch,\n",
    "        utterances_per_speaker,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # FIXME: currently, the gradient is None if loss_device is cuda\n",
    "    loss_device = torch.device(\"cpu\")\n",
    "    model = GRUVoiceEncoder(device,loss_device) \n",
    "    optimizer = Ranger(model.parameters(),lr=1e-4) \n",
    "    init_step=1\n",
    "    if not force_restart:\n",
    "            if state_fpath.exists():\n",
    "                print(\"Found existing model \\\"%s\\\", loading it and resuming training.\" % run_id)\n",
    "                checkpoint = torch.load(state_fpath)\n",
    "                init_step = checkpoint[\"step\"]\n",
    "                model.load_state_dict(checkpoint[\"model_state\"],strict=False)\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "                optimizer.param_groups[0][\"lr\"] = 1e-5 # in training, the lr was changed to 1e-5 at 17k steps\n",
    "            else:\n",
    "                print(\"No model \\\"%s\\\" found, starting training from scratch.\" % run_id)\n",
    "    else:\n",
    "        print(\"Starting the training from scratch.\")\n",
    "    model.train()\n",
    "    # Initialize the visualization environment\n",
    "    vis = Visualizations(run_id, vis_every, server=visdom_server, disabled=no_visdom)\n",
    "    vis.log_dataset(dataset)\n",
    "    vis.log_params()\n",
    "    device_name = str(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "    vis.log_implementation({\"Device\": device_name})\n",
    "    # Training loop\n",
    "    profiler = Profiler(summarize_every=10, disabled=False)\n",
    "    for step, speaker_batch in enumerate(loader, init_step):\n",
    "        profiler.tick(\"Blocking, waiting for batch (threaded)\")\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = torch.from_numpy(speaker_batch.data).to(device)\n",
    "        sync(device)\n",
    "        profiler.tick(\"Data to %s\" % device)\n",
    "        embeds = model(inputs)\n",
    "        sync(device)\n",
    "        profiler.tick(\"Forward pass\")\n",
    "        embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n",
    "        loss, eer = model.loss(embeds_loss)\n",
    "        sync(loss_device)\n",
    "        profiler.tick(\"Loss\")\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        profiler.tick(\"Backward pass\")\n",
    "        model.do_gradient_ops()\n",
    "        optimizer.step()\n",
    "        profiler.tick(\"Parameter update\")\n",
    "\n",
    "        # Update visualizations\n",
    "        # learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        vis.update(loss.item(), eer, step)\n",
    "\n",
    "        # Draw projections and save them to the backup folder\n",
    "        if umap_every != 0 and step % umap_every == 0:\n",
    "            print(\"Drawing and saving projections (step %d)\" % step)\n",
    "            backup_dir.mkdir(exist_ok=True)\n",
    "            projection_fpath = backup_dir.joinpath(\"%s_umap_%06d.png\" % (run_id, step))\n",
    "            embeds = embeds.detach().cpu().numpy()\n",
    "            vis.draw_projections(embeds, utterances_per_speaker, step, projection_fpath)\n",
    "            vis.save()\n",
    "\n",
    "        # Overwrite the latest version of the model\n",
    "        if save_every != 0 and step % save_every == 0:\n",
    "            print(\"Saving the model (step %d)\" % step)\n",
    "            torch.save({\n",
    "                \"step\": step + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, state_fpath)\n",
    "\n",
    "        # Make a backup\n",
    "        if backup_every != 0 and step % backup_every == 0:\n",
    "            print(\"Making a backup (step %d)\" % step)\n",
    "            backup_dir.mkdir(exist_ok=True)\n",
    "            backup_fpath = backup_dir.joinpath(\"%s_bak_%06d.pt\" % (run_id, step))\n",
    "            torch.save({\n",
    "                \"step\": step + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, backup_fpath)\n",
    "\n",
    "        profiler.tick(\"Extras (visualizations, saving)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_train.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
