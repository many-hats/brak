{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "sys.path.insert(0,'./rtvc/')\n",
    "sys.path.append('./rtvc/data_objects')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import jit\n",
    "from rtvc.encoder.model import SpeakerEncoder\n",
    "from rtvc.encoder.visualizations import Visualizations\n",
    "from torch.autograd import Function\n",
    "from rtvc.encoder.data_objects import SpeakerVerificationDataLoader, SpeakerVerificationDataset\n",
    "from rtvc.encoder.params_model import *\n",
    "from rtvc.encoder.model import SpeakerEncoder\n",
    "from rtvc.encoder.preprocess import preprocess_voxceleb1, _init_preprocess_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jit implementation of [Mish (1908.08681)](https://arxiv.org/abs/1908.08681) was taken from @rwightman's excellent [repo](https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations_jit.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#from https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/activations/activations_jit.py\n",
    "\n",
    "@torch.jit.script\n",
    "def mish_jit_fwd(x):\n",
    "    return x.mul(torch.tanh(F.softplus(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@torch.jit.script\n",
    "def mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MishJitAutoFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        return mish_jit_bwd(x, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mish_jit(x, inplace=False):\n",
    "    # inplace ignored\n",
    "    return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MishJit(nn.Module):\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super(MishJit, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [Nvidia's Tacotron 2 implementation]( https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# LinearNorm,ConvNorm adapted from TacoTron 2 Implementation : https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2\n",
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "\n",
    "class ConvNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.conv.weight,\n",
    "            gain=nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        return self.conv(signal)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Li-GRU with Mish "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Li-GRUs were first introduced in [Light Gated Recurrent Units for Speaker Recognition (1803.10225)](https://arxiv.org/abs/1803.10225). The core ideas behind Li-GRUs are that removing the reset gate from GRUs would be helpful as the past state is usually always relevant in the context of speech and coupled ReLU and BatchNorm instead of *tanh*. [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi) found Li-GRU to be the best performing model on TIMIT, using a bidirectional 5-layer stack of Li-GRUs with `hidden_dim` of 550 and dropout of 0.2 between layers. \n",
    "\n",
    "Brak extends this idea to speaker encoding, but replaces the ReLU with Mish. Brak uses a 3-layer stack of Li-GRU's with `hidden_dim` of 256 and dropout of 0.2 between layers. The initial curiosity for Mish arose from its and its predecessor's ([Swish (1710.05941)](https://arxiv.org/abs/1710.05941)) similarity to an inverted low pass filter with resonance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Swish**\n",
    "\n",
    "<img src='../brak/swish.png' width=300 height=300/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low Pass Filter**\n",
    "\n",
    "<img src='../brak/low pass filter with resonance.gif'  width=300 height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Adapted from pytorch-kaldi https://github.com/mravanelli/pytorch-kaldi/blob/master/neural_networks.py\n",
    "class LiGru(nn.Module):\n",
    "    def __init__(self,inp_dim=40,layer_dims=[550,550,550,550,550], ps=0.2, \n",
    "                 activations=[''], bn_inp=False, ln_inp=False,\n",
    "                 use_ln=False, use_bn=True,bidir=True,orth_init=True,\n",
    "                ):\n",
    "        super(LiGru,self).__init__()\n",
    "\n",
    "        self.input_dim = inp_dim # n_channels\n",
    "        self.ps = ps # dropout\n",
    "        self.bidir=bidir\n",
    "        self.orthinit = orth_init\n",
    "#         self.activation = nn.ReLU()\n",
    "        self.activation = MishJit()\n",
    "        self.to_do = 'train' # training flag\n",
    "        if self.to_do == 'train':\n",
    "            self.test_flag = False\n",
    "        else:\n",
    "            self.test_flag = True\n",
    "        self.wh = nn.ModuleList([])\n",
    "        self.uh = nn.ModuleList([])\n",
    "\n",
    "        self.wz = nn.ModuleList([])  # Update Gate\n",
    "        self.uz = nn.ModuleList([])  # Update Gate\n",
    "\n",
    "        self.ln = nn.ModuleList([])  # Layer Norm\n",
    "        self.bn_wh = nn.ModuleList([])  # Batch Norm\n",
    "        self.bn_wz = nn.ModuleList([])  # Batch Norm\n",
    "#         self.act = nn.ModuleList([])  # Activations\n",
    "        self.ligru_lay = layer_dims\n",
    "        self.N_ligru_lay = len(self.ligru_lay)\n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "        self.bn_inp = bn_inp\n",
    "        self.ln_inp = ln_inp\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.bn_inp:\n",
    "        # batchnorm\n",
    "            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n",
    "    \n",
    "        if self.ln_inp:\n",
    "        # layer norm\n",
    "            self.ln0 = LayerNorm(self.input_dim)\n",
    "        current_input = self.input_dim\n",
    "        \n",
    "        # hidden inits\n",
    "        \n",
    "        for i in range(self.N_ligru_lay):\n",
    "            add_bias = True\n",
    "\n",
    "            if self.use_ln or self.use_bn:\n",
    "                add_bias = False\n",
    "\n",
    "            # Feed-forward connections\n",
    "            self.wh.append(nn.Linear(current_input, self.ligru_lay[i], bias=add_bias))\n",
    "            self.wz.append(nn.Linear(current_input, self.ligru_lay[i], bias=add_bias))\n",
    "\n",
    "            # Recurrent connections\n",
    "            self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias=False))\n",
    "            self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias=False))\n",
    "\n",
    "            if self.orthinit:\n",
    "                nn.init.orthogonal_(self.uh[i].weight)\n",
    "                nn.init.orthogonal_(self.uz[i].weight)\n",
    "\n",
    "            # batch norm initialization\n",
    "            self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i], momentum=0.05))\n",
    "            self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i], momentum=0.05))\n",
    "\n",
    "            self.ln.append(LayerNorm(self.ligru_lay[i]))\n",
    "\n",
    "            if self.bidir:\n",
    "                current_input = 2 * self.ligru_lay[i]\n",
    "            else:\n",
    "                current_input = self.ligru_lay[i]\n",
    "\n",
    "        self.out_dim = self.ligru_lay[i] + self.bidir * self.ligru_lay[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applying Layer/Batch Norm\n",
    "        if self.ln_inp:\n",
    "            x = self.ln0((x))\n",
    "\n",
    "        if self.bn_inp:\n",
    "            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n",
    "            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "        for i in range(self.N_ligru_lay):\n",
    "\n",
    "            # Initial state and concatenation\n",
    "            if self.bidir:\n",
    "                h_init = torch.zeros(2 * x.shape[1], self.ligru_lay[i])\n",
    "                x = torch.cat([x, torch.flip(x, [0])], 1)\n",
    "            else:\n",
    "                h_init = torch.zeros(x.shape[1], self.ligru_lay[i])\n",
    "\n",
    "            # Drop mask initilization (same mask for all time steps)\n",
    "            if self.test_flag == False:\n",
    "                drop_mask = torch.bernoulli(\n",
    "                    torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.ps)\n",
    "                )\n",
    "            else:\n",
    "                drop_mask = torch.FloatTensor([1 - self.ps])\n",
    "\n",
    "            if self.use_cuda:\n",
    "                h_init = h_init.cuda()\n",
    "                drop_mask = drop_mask.cuda()\n",
    "\n",
    "            # Feed-forward affine transformations (all steps in parallel)\n",
    "            wh_out = self.wh[i](x)\n",
    "            wz_out = self.wz[i](x)\n",
    "\n",
    "            # Apply batch norm if needed (all steos in parallel)\n",
    "            if self.use_bn:\n",
    "\n",
    "                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n",
    "                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n",
    "\n",
    "                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n",
    "                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n",
    "\n",
    "            # Processing time steps\n",
    "            hiddens = []\n",
    "            ht = h_init\n",
    "\n",
    "            for k in range(x.shape[0]):\n",
    "\n",
    "                # ligru equation\n",
    "                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht))\n",
    "                at = wh_out[k] + self.uh[i](ht)\n",
    "                hcand = self.activation(at) * drop_mask\n",
    "                ht = zt * ht + (1 - zt) * hcand\n",
    "\n",
    "                if self.use_ln:\n",
    "                    ht = self.ln[i](ht)\n",
    "\n",
    "                hiddens.append(ht)\n",
    "\n",
    "            # Stacking hidden states\n",
    "            h = torch.stack(hiddens)\n",
    "\n",
    "            # Bidirectional concatenations\n",
    "            if self.bidir:\n",
    "                h_f = h[:, 0 : int(x.shape[1] / 2)]\n",
    "                h_b = torch.flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), [0])\n",
    "                h = torch.cat([h_f, h_b], 2)\n",
    "\n",
    "            # Setup x for the next hidden layer\n",
    "            x = h\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# subclasses resemblyzer SpeakerEncoder\n",
    "class GRUVoiceEncoder(SpeakerEncoder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(GRUVoiceEncoder,self).__init__(*args, **kwargs)\n",
    "        self.lstm=None\n",
    "        self.gru = LiGru(input_size,[hidden_dim]*n_layers,ps=0.2,bidir=False).to(device)\n",
    "        self.relu = None\n",
    "        self.mish = MishJit()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=hidden_dim, \n",
    "                                out_features=output_dim).to(device)\n",
    "    def forward(self, utterances):\n",
    "        \"\"\"\n",
    "        Computes the embeddings of a batch of utterance spectrograms.\n",
    "        \n",
    "        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape \n",
    "        (batch_size, n_frames, n_channels) \n",
    "        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers, \n",
    "        batch_size, hidden_size). Will default to a tensor of zeros if None.\n",
    "        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n",
    "        # and the final cell state.\n",
    "        hn = self.gru(utterances)\n",
    "        # We take only the hidden state of the last layer\n",
    "        embeds_raw = self.mish(self.linear(hn[:,-1,:]))\n",
    "        # L2-normalize it\n",
    "        embeds = embeds_raw / torch.norm(embeds_raw, dim=-1, keepdim=True)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_model.ipynb.\n",
      "Converted 03_train.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
